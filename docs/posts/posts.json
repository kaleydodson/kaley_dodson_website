[
  {
    "path": "posts/2021-02-24-chloropleth/",
    "title": "Chloropleth Map",
    "description": "Spatial data visualization using ggplot.",
    "author": [],
    "date": "2021-02-27",
    "categories": [],
    "contents": "\n\nShow code\n# read in data\n\nca_counties <- read_sf(here(\"data\", \"ca_counties\"), layer = \"CA_Counties_TIGER2016\") %>% \n  clean_names() %>% \n  select(name)\n\noil_spills <- read_sf(here(\"data\", \"ds394\"), layer = \"ds394\") %>% \n  clean_names() \n\n# check projection\nst_crs(ca_counties)\nst_crs(oil_spills)\n\nca_counties <- st_transform(ca_counties, st_crs(oil_spills)) # transforms to same crs as oil spill dataset\n\n\n\nHere I created a static map in ggplot illustrating inland oil spill events by county. After joining spatial data for CA counties with a dataset for oil spills in the state, I used geom_sf() to create a chloropleth map in which the fill color depended on the count of inland oil spill events by county.\n\nShow code\n# combine data sets to bid spill counts to counties\ncounty_spills <- ca_counties %>% \n  st_join(oil_spills)\n\nspills_count <- county_spills %>% \n  filter(inlandmari == \"Inland\") %>% \n  count(name)\n\nggplot(data = spills_count) +\n  geom_sf(aes(fill = n), color = \"lightgray\", size = 0.2) +\n  scale_fill_gradientn(colors = c(\"white\",\"orange\",\"red\")) +\n  theme_minimal() +\n  labs(fill = \"Number of inland oil spills\")\n\n\n\n\nFigure 1: Figure 1: Total inland oil spills by county in 2008\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-24-chloropleth/chloropleth_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-27T22:41:50-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-27-raster/",
    "title": "Working With Rasters",
    "description": "Visualizing raster data in R.",
    "author": [],
    "date": "2021-02-27",
    "categories": [],
    "contents": "\n\nShow code\n# read in data\n\ncetacean_richness <- here(\"data\", \"ca_cetaceans\")\n\ncetaceans <- dir(cetacean_richness, full.names = TRUE, pattern = \"*.tif\")\n\ncetaceans_stack <- stack(cetaceans)\n\n# threshold function\nis_present <- function(x, thresh = .75) {\n  y <- ifelse(x >= thresh, 1, NA)\n  return(y)\n}\n\n# calculating species richness \ncetaceans_richness <- calc(cetaceans_stack, fun = is_present)\n\nspecies_richness <- calc(cetaceans_richness, fun = sum, na.rm = TRUE)\n\n\n\nHere I read in a raster stack and wrote a function to determine the presence of cetacean speices. After applying the function, I calculated overall cetacean species richness and then converted the raster to a dataframe. Finally, I visualized the result by layering raster data with spatial data from the rnaturalearthdata package.\n\nShow code\n# download ca coastline layer from rnaturalearth\ncoastline <- ne_download(scale = \"medium\", type = 'land', category = 'physical', returnclass = \"sf\")\n\n# convert raster to data frame\nspecies_richness_df <- raster::rasterToPoints(species_richness) %>% \n  as.data.frame()\n\nggplot() +\n  geom_raster(data = species_richness_df, aes(x = x, y = y, fill = layer)) +\n  geom_sf(data = coastline, fill = \"lavender\") +\n  coord_sf(xlim = c(-125,-115), ylim = c(33,37)) +\n  scale_fill_gradient(low = \"skyblue4\", high = \"seagreen3\") +\n  theme_void() \n\n\n\n\nFigure 1: Map of cetacean species richness in the California Bight, using a threshold of 75% probability of occurence to determine presence for each species.\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-27-raster/raster_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-27T23:11:04-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-27-timeseries/",
    "title": "Timeseries Plot",
    "description": "Wrangling and visualizing time series data.",
    "author": [],
    "date": "2021-02-27",
    "categories": [],
    "contents": "\n\nShow code\n# read in data \nwillamette_fish <- read.csv(here(\"data\", \"willamette_fish_passage.csv\")) %>% \n  clean_names() \n\n#data wrangling, turn dataset into a tsibble\n\nfish_ts <- willamette_fish %>% \n   pivot_longer( # tidy's data by consolidating steelhead, coho, and jack_coho columns called species into one and their respective observations into a second colunm called fish_count\n    cols = c(steelhead, coho, jack_coho),\n    names_to = \"species\",\n    values_to = \"fish_count\"\n  ) %>% \n  mutate(date = mdy(date)) %>% # converts date to y-m-d format\n  as_tsibble(key = species, index = date) %>% # converts data to tsibble\n  select(date, species, fish_count) %>% # retains only date, species, and fish_count variables\n  mutate(species_name = case_when( # creates new column with species' full names\n    species == \"coho\" ~ \"Coho\",\n    species == \"jack_coho\" ~ \"Jack Coho\",\n    species == \"steelhead\" ~ \"Steelhead\"\n  ))\n\n# wrangling to aggregate fish counts by year\nfish_annual <- fish_ts %>% \n  index_by(year = ~year(.)) %>% # groups observations by year\n  group_by(species_name) %>% # groups observations by species\n  summarize(total_count = sum(fish_count, na.rm = TRUE)) # counts observations per year by species\n\n\n\nAs part of a report exploring data for adult fish passage recorded from 2001-01-01 to 2010-12-31 on the Willamette River in Oregon, I worked with time series data. After tidying the data using the pivot_longer() function and converting to a tsibble, I wrangled to aggregate counts by year and produced the following summary plot.\n\nShow code\n#vizualization of annual counts\nggplot(data = fish_annual, aes(x = year, y = total_count, fill = species_name)) +\n  geom_col(show.legend = FALSE, color = \"#317eb1\") +\n  facet_wrap(~species_name, scales = \"free\") +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Individuals Observed\") +\n  scale_x_continuous(breaks = c(2002, 2004, 2006, 2008, 2010)) +\n  theme(axis.text.x = element_text(angle = 30)) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_fill_brewer(palette = \"Blues\")\n\n\n\n\nFigure 1: Figure 3: Annual salmon counts at the Willamette Falls fish ladder for steelhead and coho species. (Data: Columbia River DART)\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-27-timeseries/timeseries_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-27T23:26:16-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-27-txtanalysis/",
    "title": "Text Analysis",
    "description": "Sentiment analysis using the NRC lexicon.",
    "author": [],
    "date": "2021-02-27",
    "categories": [],
    "contents": "\n\nShow code\n# load text data\nfl_text <- readtext(here(\"data\", \"fear_and_loathing.txt\"))\n\n#wrangling\nfl_tidy <- fl_text %>% \n  mutate(text = str_split(fl_text, pattern = \"\\\\n\")) %>% # splits full text up by line breaks\n  unnest(text) %>% # moves each line of text out to its own row\n  slice(-(1:17)) %>% # removes title pages\n  mutate(text = str_to_lower(text)) %>% # converts all text to lowercase\n  unnest_tokens(word, text) # moves each word out into its own row\n  \nfl_nonstop_words <- fl_tidy %>% \n  anti_join(stop_words) # removes stop words\n\nnonstop_counts <- fl_nonstop_words %>% \n  count(word) # counts total use of each word\n\nfl_nrc <- fl_nonstop_words %>% \n  inner_join(get_sentiments(\"nrc\")) # joins text and nrc dataframes\n\nfl_nrc_counts <- fl_nrc %>% \n  count(sentiment) # counts words that fall into each sentiment category\n\n\n\nI wrangled and visualized the top 100 words that appear in the novel Fear and Loathing in Las Vegas, then joined the data frame with the nrc lexicon before counting words associated with its sentiment bins and visualizing the result.\n\nShow code\n# visualization\nggplot(data = fl_nrc_counts, aes(x = sentiment, y = n)) +\n  geom_col(fill = rainbow(10)) +\n  coord_flip() +\n  theme_classic() +\n  labs(y = \" \", x = \"nrc sentiment\")\n\n\n\n\nCitation: Thompson, Hunter S, and Ralph Steadman. Fear and Loathing in Las Vegas: A Savage Journey to the Heart of the American Dream. , 1998.\n\n\n\n",
    "preview": "posts/2021-02-27-txtanalysis/txtanalysis_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-27T22:59:12-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-24-tidytues/",
    "title": "Tidy Tuesday",
    "description": "Mapping spatial data with tmap.",
    "author": [],
    "date": "2021-02-11",
    "categories": [],
    "contents": "\n\nShow code\ntuesdata <- tidytuesdayR::tt_load(2021, week = 5)\n\nplastics <- tuesdata$plastics\n\ndata(World)\n\nworld_lat_long <- read.csv(here(\"data\", \"world_country_and_usa_states_latitude_and_longitude_values.csv\")) %>% \n  select(country, latitude, longitude) \n\n# wrangling for total plastic per country\nplastics_subset <- plastics %>% \n  filter(parent_company == \"Grand Total\")\n\nplastics_subset[51, 1] = \"United States\"\nplastics_subset[33, 1] = \"Nigeria\"\nplastics_subset[15, 1] = \"Ecuador\"\nplastics_subset[43, 1] = \"Taiwan\"\n\n# joining with lat & long coordinates\nmerged_data <- left_join(plastics_subset, world_lat_long, by = \"country\") \n\n# convert to lat and long to spatial coordinates\nplastics_sp <- merged_data %>% \n  drop_na(longitude, latitude) %>% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"))\n\nst_crs(plastics_sp) = 4326\n\n\n\nI made a basic map of total plastic pollution by country using: 1) dplyr join function to merge lat/long coords from downloaded data; and 2) tmap package to map countriesâ€™ plastic data onto a world map.\nI renamed some countries in the plastics data so they would match my downloaded dataset.\n\nShow code\n# map\ntm_shape(World) +\n  tm_borders(\"thistle\", lwd = .5) +\ntm_shape(plastics_sp) +\n  tm_bubbles(size = \"grand_total\") \n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-24-tidytues/tidytues_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-27T22:45:13-08:00",
    "input_file": {}
  }
]
